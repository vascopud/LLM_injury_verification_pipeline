{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f0f3ff",
   "metadata": {},
   "source": [
    "Evaluation of Mistral Large Injury Verification\n",
    "\n",
    "This notebook evaluates the LLM outputs stored in the file generated by\n",
    "the pipeline notebook.\n",
    "\n",
    "Expected columns:\n",
    "\n",
    "- `human_injury`, `mistral_injury`\n",
    "- `human_surgery`, `mistral_surgery`\n",
    "- `human_partial`, `mistral_partial`\n",
    "- `human_associated_injuries`, `mistral_associated_injuries`\n",
    "- `human_text_class`, `mistral_text_class`\n",
    "\n",
    "We compute accuracy, precision, recall (sensitivity), specificity,\n",
    "F1, confusion matrices, and a multi-class report for text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the CSV produced by the pipeline notebook\n",
    "CSV_PATH = \"appraisal_results_01.csv\" # or \"results/appraisal_results_01.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, index_col=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for a binary task. (Assumes labels 0/1 for both human and model columns.)\n",
    "def binary_metrics(df, human_col, mistral_col):\n",
    "    # Keep only rows with usable predictions\n",
    "    valid_mask = df[mistral_col].isin([0, 1, \"0\", \"1\"])\n",
    "    filtered = df[valid_mask].copy()\n",
    "\n",
    "    if len(filtered) == 0:\n",
    "        raise ValueError(f\"No valid rows to evaluate for {mistral_col} (all 'Skipped' or missing).\")\n",
    "\n",
    "    # Convert everything to int\n",
    "    y_true = filtered[human_col].astype(int).to_numpy()\n",
    "    y_pred = filtered[mistral_col].astype(int).to_numpy()\n",
    "\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)  # sensitivity\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm   = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"n_samples\": len(filtered),\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall_sensitivity\": rec,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1\": f1,\n",
    "        \"tn\": int(tn),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "        \"tp\": int(tp),\n",
    "        \"confusion_matrix\": cm,\n",
    "    }\n",
    "    return metrics, filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ad1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title=\"Confusion matrix\", labels=(\"0\", \"1\")):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, int(v), ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.colorbar(im)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"injury\":              (\"human_injury\",              \"mistral_injury\"),\n",
    "    \"surgery\":             (\"human_surgery\",             \"mistral_surgery\"),\n",
    "    \"partial\":             (\"human_partial\",             \"mistral_partial\"),\n",
    "    \"associated_injuries\": (\"human_associated_injuries\", \"mistral_associated_injuries\"),\n",
    "}\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for name, (y_col, yhat_col) in tasks.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "\n",
    "    m, df_used = binary_metrics(df, y_col, yhat_col)\n",
    "\n",
    "    print(f\"Samples used (non-skipped): {m['n_samples']}\")\n",
    "    print(f\"Accuracy:            {m['accuracy']:.4f}\")\n",
    "    print(f\"Precision:           {m['precision']:.4f}\")\n",
    "    print(f\"Recall (Sensitivity):{m['recall_sensitivity']:.4f}\")\n",
    "    print(f\"Specificity:         {m['specificity']:.4f}\")\n",
    "    print(f\"F1-score:            {m['f1']:.4f}\")\n",
    "    print(f\"TP: {m['tp']} | FP: {m['fp']} | TN: {m['tn']} | FN: {m['fn']}\")\n",
    "\n",
    "    plot_confusion_matrix(m[\"confusion_matrix\"], title=f\"{name} â€“ confusion matrix\")\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"task\": name,\n",
    "        \"n_samples\": m[\"n_samples\"],\n",
    "        \"accuracy\": m[\"accuracy\"],\n",
    "        \"precision\": m[\"precision\"],\n",
    "        \"recall_sensitivity\": m[\"recall_sensitivity\"],\n",
    "        \"specificity\": m[\"specificity\"],\n",
    "        \"f1\": m[\"f1\"],\n",
    "        \"tp\": m[\"tp\"],\n",
    "        \"fp\": m[\"fp\"],\n",
    "        \"tn\": m[\"tn\"],\n",
    "        \"fn\": m[\"fn\"],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27827455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out Skipped / invalid predictions (if any, shouldn't be necessary)\n",
    "valid_mask_text = df[\"mistral_text_class\"].isin([0, 1, 2, 3, 4, \"0\", \"1\", \"2\", \"3\", \"4\"])\n",
    "df_text = df[valid_mask_text].copy()\n",
    "\n",
    "y_true_text = df_text[\"human_text_class\"].astype(int)\n",
    "y_pred_text = df_text[\"mistral_text_class\"].astype(int)\n",
    "\n",
    "print(\"=== TEXT CLASSIFICATION ===\")\n",
    "print(f\"Samples used (non-skipped): {len(df_text)}\")\n",
    "print(\"Overall accuracy:\", accuracy_score(y_true_text, y_pred_text))\n",
    "\n",
    "print(\"\\nClassification report (per class):\\n\")\n",
    "print(classification_report(y_true_text, y_pred_text, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e169756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verified = df[df[\"human_injury\"] == 1].copy()\n",
    "\n",
    "print(f\"Number of rows with human_injury == 1: {len(df_verified)}\")\n",
    "\n",
    "tasks_verified = {\n",
    "    \"surgery_given_injury\":             (\"human_surgery\",             \"mistral_surgery\"),\n",
    "    \"partial_given_injury\":             (\"human_partial\",             \"mistral_partial\"),\n",
    "    \"assoc_injuries_given_injury\":      (\"human_associated_injuries\", \"mistral_associated_injuries\"),\n",
    "}\n",
    "\n",
    "rows_verified = []\n",
    "for name, (y_col, yhat_col) in tasks_verified.items():\n",
    "    print(f\"\\n=== {name.upper()} (subset: human_injury == 1) ===\")\n",
    "    m, df_used = binary_metrics(df_verified, y_col, yhat_col)\n",
    "    print(f\"Samples used:        {m['n_samples']}\")\n",
    "    print(f\"Accuracy:            {m['accuracy']:.4f}\")\n",
    "    print(f\"Precision:           {m['precision']:.4f}\")\n",
    "    print(f\"Recall (Sensitivity):{m['recall_sensitivity']:.4f}\")\n",
    "    print(f\"Specificity:         {m['specificity']:.4f}\")\n",
    "    print(f\"F1-score:            {m['f1']:.4f}\")\n",
    "    print(f\"TP: {m['tp']} | FP: {m['fp']} | TN: {m['tn']} | FN: {m['fn']}\")\n",
    "\n",
    "    rows_verified.append({\"task\": name, **{k: v for k, v in m.items() if k != \"confusion_matrix\"}})\n",
    "\n",
    "pd.DataFrame(rows_verified)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
